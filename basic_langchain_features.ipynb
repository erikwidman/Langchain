{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b027458c",
   "metadata": {},
   "source": [
    "# Basic Langchain Features\n",
    "\n",
    "This notebook covers some of the fundamental Features of Langchain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "560c31f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import credentials\n",
    "\n",
    "#set environment variable for OpenAI API Key\n",
    "os.environ[\"OPENAI_API_KEY\"] = credentials.llm_api_key"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73059922",
   "metadata": {},
   "source": [
    "## Call a LLM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d7f06a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Abisko National Park\n",
      "2. Koster Islands\n",
      "3. Gotland\n",
      "4. Jokkmokk\n",
      "5. Lulea Archipelago\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.5)\n",
    "text = \"What are 5 vacation destinations in sweden for someboy who likes nature?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3681c618",
   "metadata": {},
   "source": [
    "## Prompt Tempplates\n",
    "LLMs take promps as inputs. Typically this is not simply a hardcoded string but rather a combination of a template, some examples, and user input. \n",
    "\n",
    "A prompt template refers to a reproducible way to generate a prompt. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt.\n",
    "\n",
    "A prompt template can contain:\n",
    "\n",
    "* instructions to the language model,\n",
    "* a set of few shot examples to help the language model generate a better response,\n",
    "* a question to the language model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d5be12ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"sport\"],\n",
    "    template=\"What are 5 vacation destinations for someone who likes {sport}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40d41b2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What are 5 vacation destinations for someone who likes basketball?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(sport=\"basketball\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "439d1f1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. New York City, USA - Home of the NBA's New York Knicks and Brooklyn Nets.\n",
      "2. Los Angeles, USA - Home of the NBA's Los Angeles Lakers and Los Angeles Clippers.\n",
      "3. Chicago, USA - Home of the NBA's Chicago Bulls.\n",
      "4. Toronto, Canada - Home of the NBA's Toronto Raptors.\n",
      "5. Las Vegas, USA - Host of the NBA Summer League.\n"
     ]
    }
   ],
   "source": [
    "print(llm(prompt.format(sport=\"basketball\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd81ee21",
   "metadata": {},
   "source": [
    "## Chains: Combine multiple LLMs and prompts in multi-step workflows\n",
    "Using an LLM in isolation is fine for simple applications, but more complex applications require chaining LLMs - either with each other or with other components.\n",
    "\n",
    "We define a Chain very generically as a sequence of calls to components, which can include other chains. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb627f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import LLMChain\n",
    "\n",
    "llm = OpenAI(temperature=0.5)\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"food\"],\n",
    "    template=\"What are 5 vacation destinations for someone who likes to eat {food}?\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d7f3c6a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "1. Costa Rica - known for its abundance of tropical fruits like pineapple, coconut, papaya, and mango.\n",
      "\n",
      "2. Hawaii - known for its delicious tropical fruits like pineapple, guava, lychee, and papaya.\n",
      "\n",
      "3. Thailand - known for its exotic fruits like durian, mangosteen, lychee, and rambutan.\n",
      "\n",
      "4. Jamaica - known for its sweet and juicy fruits like bananas, mangoes, papayas, and oranges.\n",
      "\n",
      "5. India - known for its unique and flavorful fruits like lychee, guava, jackfruit, and mango.\n"
     ]
    }
   ],
   "source": [
    "#chain the LLM and the prompt. We are not manually feeding the prompt into the language model\n",
    "chain = LLMChain(llm=llm, prompt=prompt)\n",
    "print(chain.run(\"fruit\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a404d75",
   "metadata": {},
   "source": [
    "Let's try chaining the chat module in a LLM chain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e1f9a533",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ColorfulStride\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (ChatPromptTemplate, HumanMessagePromptTemplate)\n",
    "\n",
    "human_message_prompt = HumanMessagePromptTemplate(\n",
    "        prompt=PromptTemplate(\n",
    "            template=\"What is a good name for a company that makes {product}?\",\n",
    "            input_variables=[\"product\"],\n",
    "        )\n",
    "    )\n",
    "\n",
    "chat_prompt_template = ChatPromptTemplate.from_messages([human_message_prompt])\n",
    "chat = ChatOpenAI(temperature=0.5)\n",
    "\n",
    "#chain chat and prompt together\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt_template)\n",
    "print(chain.run(\"colorful socks\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a6e3e83",
   "metadata": {},
   "source": [
    "## Agents: Dynamically call chains based on user input\n",
    "\n",
    "Some applications require a flexible chain of calls to LLMs and other tools based on user input. The Agent interface provides the flexibility for such applications. An agent has access to a suite of tools, and determines which ones to use depending on the user input. Agents can use multiple tools, and use the output of one tool as the input to the next.\n",
    "\n",
    "There are two main types of agents:\n",
    "\n",
    "* Action agents: at each timestep, decide on the next action using the outputs of all previous actions\n",
    "* Plan-and-execute agents: decide on the full sequence of actions up front, then execute them all without updating the plan\n",
    "\n",
    "Action agents are suitable for small tasks, while plan-and-execute agents are better for complex or long-running tasks that require maintaining long-term objectives and focus. Often the best approach is to combine the dynamism of an action agent with the planning abilities of a plan-and-execute agent by letting the plan-and-execute agent use action agents to execute plans.\n",
    "\n",
    "https://python.langchain.com/docs/modules/agents/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8f424069",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# Load the model\n",
    "llm = OpenAI(temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7739e98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load tools for the agent to use\n",
    "#load serpapi api key. Serpapi let's us scrape google searches\n",
    "os.environ[\"SERPAPI_API_KEY\"] = credentials.serpapi_api_key\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "21193883",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's initialize an agent with:\n",
    "# 1. The tools\n",
    "# 2. The language model\n",
    "# 3. The type of agent we want to use.\n",
    "\n",
    "agent = initialize_agent(tools, llm, agent=\"zero-shot-react-description\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d581dc00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out who the leader of Sweden is and then calculate the largest prime number that is smaller than their age.\n",
      "Action: Search\n",
      "Action Input: \"leader of Sweden\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mUlf Kristersson\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out the age of Ulf Kristersson\n",
      "Action: Search\n",
      "Action Input: \"Ulf Kristersson age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m59 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to calculate the largest prime number that is smaller than 59\n",
      "Action: Calculator\n",
      "Action Input: 59\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 59\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Ulf Kristersson is the current leader of Sweden and the largest prime number that is smaller than their age is 59.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Ulf Kristersson is the current leader of Sweden and the largest prime number that is smaller than their age is 59.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run agent\n",
    "agent.run(\"Who is the current leader of Sweden? What is the largest prime number that is smaller than their age?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d3d8122",
   "metadata": {},
   "source": [
    "Let's try another math based prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "60777450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out the ages of both Joe Biden and Donald Trump.\n",
      "Action: Search\n",
      "Action Input: \"Joe Biden age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m80 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I need to find out Donald Trump's age\n",
      "Action: Search\n",
      "Action Input: \"Donald Trump age\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m77 years\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the ages of both Joe Biden and Donald Trump\n",
      "Action: Calculator\n",
      "Action Input: (80 + 77) / 2\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 78.5\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: The average age of Joe Biden and Donald Trump is 78.5 years.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The average age of Joe Biden and Donald Trump is 78.5 years.'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run agent\n",
    "agent.run(\"What is the average age of joe biden and donald trump?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "df45c095",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3m I need to find out how many guitar stores are in Chicago.\n",
      "Action: Search\n",
      "Action Input: \"guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this list to count the number of stores.\n",
      "Action: Count\n",
      "Action Input: Number of stores on the list\u001b[0m\n",
      "Observation: Count is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to count the number of stores.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to count the number of stores.\n",
      "Action: Count\n",
      "Action Input: Number of stores on the list\u001b[0m\n",
      "Observation: Count is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to find the answer.\n",
      "Action: Read\n",
      "Action Input: The search result\u001b[0m\n",
      "Observation: Read is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to find the answer.\n",
      "Action: Analyze\n",
      "Action Input: The search result\u001b[0m\n",
      "Observation: Analyze is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to find the answer.\n",
      "Action: Read\n",
      "Action Input: The search result\u001b[0m\n",
      "Observation: Read is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to find the answer.\n",
      "Action: Count\n",
      "Action Input: Number of stores on the list\u001b[0m\n",
      "Observation: Count is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use this search result to find the answer.\n",
      "Action: Read\n",
      "Action Input: The search result\u001b[0m\n",
      "Observation: Read is not a valid tool, try another one.\n",
      "Thought:\u001b[32;1m\u001b[1;3m I can use the search engine to find the answer.\n",
      "Action: Search\n",
      "Action Input: \"number of guitar stores in Chicago\"\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mhttps://www.yelp.com/search?cflt=guitarstores&find_loc=Chicago%2C+IL\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Agent stopped due to iteration limit or time limit.'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Run agent\n",
    "agent.run(\"How many guitar stores are there in Chicago?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a57f056a",
   "metadata": {},
   "source": [
    "### Agent that searches csv file\n",
    "\n",
    "Let's try to querry a college basketball statistics csv file.\n",
    "\n",
    "https://www.kaggle.com/datasets/andrewsundberg/college-basketball-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b8c8beaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.agents import create_csv_agent\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.agents.agent_types import AgentType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a8721a0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_csv_agent(\n",
    "    OpenAI(temperature=0),\n",
    "    \"cbb.csv\",\n",
    "    verbose=True,\n",
    "    agent_type=AgentType.ZERO_SHOT_REACT_DESCRIPTION,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "223278c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to get the descriptive statistics for the data and check for missing data.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.describe()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m                 G            W        ADJOE        ADJDE      BARTHAG  \\\n",
      "count  2455.000000  2455.000000  2455.000000  2455.000000  2455.000000   \n",
      "mean     31.492464    16.284318   103.304481   103.304603     0.493957   \n",
      "std       2.657401     6.610960     7.376981     6.605318     0.256244   \n",
      "min      15.000000     0.000000    76.600000    84.000000     0.005000   \n",
      "25%      30.000000    11.000000    98.300000    98.500000     0.282200   \n",
      "50%      31.000000    16.000000   103.000000   103.500000     0.475000   \n",
      "75%      33.000000    21.000000   108.000000   107.900000     0.712200   \n",
      "max      40.000000    38.000000   129.100000   124.000000     0.984200   \n",
      "\n",
      "             EFG_O        EFG_D          TOR         TORD          ORB  ...  \\\n",
      "count  2455.000000  2455.000000  2455.000000  2455.000000  2455.000000  ...   \n",
      "mean     49.805132    50.001385    18.763055    18.689572    29.875642  ...   \n",
      "std       3.143061     2.939602     2.090595     2.201749     4.134332  ...   \n",
      "min      39.200000    39.600000    11.900000    10.200000    15.000000  ...   \n",
      "25%      47.750000    48.000000    17.300000    17.200000    27.100000  ...   \n",
      "50%      49.700000    50.000000    18.700000    18.600000    29.900000  ...   \n",
      "75%      51.900000    52.000000    20.100000    20.100000    32.600000  ...   \n",
      "max      59.800000    59.500000    27.100000    28.500000    43.600000  ...   \n",
      "\n",
      "               FTR        FTRD         2P_O         2P_D         3P_O  \\\n",
      "count  2455.000000  2455.00000  2455.000000  2455.000000  2455.000000   \n",
      "mean     35.989776    36.26998    48.802281    48.976660    34.406395   \n",
      "std       5.247820     6.24590     3.384468     3.340546     2.789434   \n",
      "min      21.600000    21.80000    37.700000    37.700000    24.900000   \n",
      "25%      32.400000    31.90000    46.500000    46.700000    32.500000   \n",
      "50%      35.800000    35.80000    48.700000    49.000000    34.400000   \n",
      "75%      39.500000    40.20000    51.000000    51.300000    36.300000   \n",
      "max      58.600000    60.70000    62.600000    61.200000    44.100000   \n",
      "\n",
      "              3P_D        ADJ_T          WAB        SEED         YEAR  \n",
      "count  2455.000000  2455.000000  2455.000000  476.000000  2455.000000  \n",
      "mean     34.598737    67.812301    -7.802485    8.802521  2016.007332  \n",
      "std       2.415766     3.277622     6.965736    4.676354     1.999375  \n",
      "min      27.100000    57.200000   -25.200000    1.000000  2013.000000  \n",
      "25%      33.000000    65.700000   -13.000000    5.000000  2014.000000  \n",
      "50%      34.600000    67.800000    -8.300000    9.000000  2016.000000  \n",
      "75%      36.200000    70.000000    -3.150000   13.000000  2018.000000  \n",
      "max      43.100000    83.400000    13.100000   16.000000  2019.000000  \n",
      "\n",
      "[8 rows x 21 columns]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I have the descriptive statistics for my data. Now I need to check for missing data.\n",
      "Action: python_repl_ast\n",
      "Action Input: df.isnull().sum()\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mTEAM             0\n",
      "CONF             0\n",
      "G                0\n",
      "W                0\n",
      "ADJOE            0\n",
      "ADJDE            0\n",
      "BARTHAG          0\n",
      "EFG_O            0\n",
      "EFG_D            0\n",
      "TOR              0\n",
      "TORD             0\n",
      "ORB              0\n",
      "DRB              0\n",
      "FTR              0\n",
      "FTRD             0\n",
      "2P_O             0\n",
      "2P_D             0\n",
      "3P_O             0\n",
      "3P_D             0\n",
      "ADJ_T            0\n",
      "WAB              0\n",
      "POSTSEASON    1979\n",
      "SEED          1979\n",
      "YEAR             0\n",
      "dtype: int64\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer.\n",
      "Final Answer: There is no missing data in the dataframe. The descriptive statistics for the dataframe are provided in the observation above.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'There is no missing data in the dataframe. The descriptive statistics for the dataframe are provided in the observation above.'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"Can you provide the descriptive statistics for my data? Is there any data missing?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "6c7a4a35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to find the team with the highest ORB value\n",
      "Action: python_repl_ast\n",
      "Action Input: df[df['YEAR'] >= 2015].sort_values(by='ORB', ascending=False).head(1)\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3m           TEAM  CONF   G   W  ADJOE  ADJDE  BARTHAG  EFG_O  EFG_D   TOR  ...  \\\n",
      "682  Quinnipiac  MAAC  30  15  101.4  101.3   0.5029   44.0   45.2  20.4  ...   \n",
      "\n",
      "     FTRD  2P_O  2P_D  3P_O  3P_D  ADJ_T   WAB  POSTSEASON  SEED  YEAR  \n",
      "682  33.4  43.1  41.9  30.7  35.1   66.7 -10.9         NaN   NaN  2015  \n",
      "\n",
      "[1 rows x 24 columns]\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3m I now know the final answer\n",
      "Final Answer: Quinnipiac had the most offensive rebounds from 2015 to 2019.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Quinnipiac had the most offensive rebounds from 2015 to 2019.'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agent.run(\"What team had the most offensive rebounds (ORB) 2015 to 2019?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9084b15a",
   "metadata": {},
   "source": [
    "## Memory - send conversation history to agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f7bc5edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c64f2337",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: What's up? My name is Erik.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi Erik! My name is AI. It's nice to meet you. I'm doing great. How about you?\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What's up? My name is Erik.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "34946a96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What's up? My name is Erik.\n",
      "AI:  Hi Erik! My name is AI. It's nice to meet you. I'm doing great. How about you?\n",
      "Human: I'm doing well! Can you tell me how many guitar stores there are in Chicago?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" I'm sorry, Erik, I don't know the answer to that question.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Can you tell me how many guitar stores there are in Chicago?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "73499f2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new  chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: What's up? My name is Erik.\n",
      "AI:  Hi Erik! My name is AI. It's nice to meet you. I'm doing great. How about you?\n",
      "Human: I'm doing well! Can you tell me how many guitar stores there are in Chicago?\n",
      "AI:  I'm sorry, Erik, I don't know the answer to that question.\n",
      "Human: What was the first thing I said to you?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' You said, \"What\\'s up? My name is Erik.\"'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"What was the first thing I said to you?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d7e168e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
